{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tda\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skfeature.function.information_theoretical_based import MRMR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('T2.csv')\n",
    "X = df.drop(['Class'], axis=1)\n",
    "Y=  df['Class']\n",
    "X11=X\n",
    "df2=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cf = pd.read_csv('radiomics_t2.csv')\n",
    "#cX = cf.drop(['Class'], axis=1)\n",
    "#cY=  cf['Class']\n",
    "#cX1=cX\n",
    "#cf2=cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ef = pd.concat([X1,cX1],axis=1)\n",
    "#ef.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = X.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.85)]\n",
    "X1=X.drop(X[to_drop], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.110500</td>\n",
       "      <td>7.42070</td>\n",
       "      <td>0.333350</td>\n",
       "      <td>0.92052</td>\n",
       "      <td>34.05800</td>\n",
       "      <td>9.605900e-01</td>\n",
       "      <td>3.32300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.057200</td>\n",
       "      <td>6.67440</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.68579</td>\n",
       "      <td>22.46400</td>\n",
       "      <td>3.963300e+00</td>\n",
       "      <td>1.88090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.972200</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.893130</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.968900e+00</td>\n",
       "      <td>0.99980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.133000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.88022</td>\n",
       "      <td>0.88022</td>\n",
       "      <td>3.092900e+00</td>\n",
       "      <td>0.77464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.876400</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.860700e+00</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27.023000</td>\n",
       "      <td>11.85400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.81479</td>\n",
       "      <td>49.62100</td>\n",
       "      <td>3.472000e+00</td>\n",
       "      <td>2.14100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.920300</td>\n",
       "      <td>8.99910</td>\n",
       "      <td>0.935850</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>60.99300</td>\n",
       "      <td>3.276100e+00</td>\n",
       "      <td>3.91360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.058100</td>\n",
       "      <td>0.99901</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.73900</td>\n",
       "      <td>2.80710</td>\n",
       "      <td>3.663600e+00</td>\n",
       "      <td>1.97240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.077000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.975760</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>3.624900e+00</td>\n",
       "      <td>0.99980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.106000</td>\n",
       "      <td>10.95100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.85632</td>\n",
       "      <td>55.02500</td>\n",
       "      <td>3.882700e+00</td>\n",
       "      <td>2.83960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.118500</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.461500e+00</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8.444900</td>\n",
       "      <td>5.38060</td>\n",
       "      <td>0.959190</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>22.73400</td>\n",
       "      <td>3.529000e+00</td>\n",
       "      <td>3.87770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.99988</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.89923</td>\n",
       "      <td>3.53180</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>3.11840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12.863000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.718000e+00</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10.470000</td>\n",
       "      <td>10.99600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.90250</td>\n",
       "      <td>59.98500</td>\n",
       "      <td>3.881300e+00</td>\n",
       "      <td>2.66790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6.993200</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.223500e+00</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.845000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.82528</td>\n",
       "      <td>0.82528</td>\n",
       "      <td>3.808500e+00</td>\n",
       "      <td>0.68095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.548700</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.65435</td>\n",
       "      <td>0.65435</td>\n",
       "      <td>3.433000e+00</td>\n",
       "      <td>0.42808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.993220</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.096500e+00</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.008602</td>\n",
       "      <td>13.68600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.98966</td>\n",
       "      <td>147.25000</td>\n",
       "      <td>9.999400e-01</td>\n",
       "      <td>3.87670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>17.282000</td>\n",
       "      <td>15.98000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.91159</td>\n",
       "      <td>183.13000</td>\n",
       "      <td>3.959500e+00</td>\n",
       "      <td>3.12030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.399300</td>\n",
       "      <td>5.16170</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.81000</td>\n",
       "      <td>13.28200</td>\n",
       "      <td>3.583300e+00</td>\n",
       "      <td>2.49760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.544700</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.90774</td>\n",
       "      <td>0.90774</td>\n",
       "      <td>3.639900e+00</td>\n",
       "      <td>0.82383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.849500</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.782150</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.402700e+00</td>\n",
       "      <td>0.99980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.026105</td>\n",
       "      <td>5.36310</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.85403</td>\n",
       "      <td>16.89500</td>\n",
       "      <td>1.000300e+00</td>\n",
       "      <td>2.78930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>16.031000</td>\n",
       "      <td>10.02900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.60236</td>\n",
       "      <td>39.01600</td>\n",
       "      <td>3.940300e+00</td>\n",
       "      <td>1.26850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>15.537000</td>\n",
       "      <td>3.84130</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>3.257400e+00</td>\n",
       "      <td>2.87070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.999110</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.630100e+00</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.99958</td>\n",
       "      <td>0.917730</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>3.86670</td>\n",
       "      <td>8.420600e-01</td>\n",
       "      <td>3.73950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>9.770900</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.622800e+00</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1.570800</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.832700e+00</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>14.546000</td>\n",
       "      <td>6.68100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.82129</td>\n",
       "      <td>28.44400</td>\n",
       "      <td>3.876400e+00</td>\n",
       "      <td>2.66690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>14.480000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.920800e+00</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>10.293000</td>\n",
       "      <td>7.87700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.74799</td>\n",
       "      <td>32.95400</td>\n",
       "      <td>3.871800e+00</td>\n",
       "      <td>2.15520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1.251300</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.747100e+00</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>9.458800</td>\n",
       "      <td>11.91100</td>\n",
       "      <td>0.886400</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>96.83400</td>\n",
       "      <td>2.777700e+00</td>\n",
       "      <td>3.91540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.014189</td>\n",
       "      <td>5.38410</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.71740</td>\n",
       "      <td>16.52900</td>\n",
       "      <td>1.000300e+00</td>\n",
       "      <td>2.05820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.320000e-09</td>\n",
       "      <td>0.99980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.147290</td>\n",
       "      <td>10.00800</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.81668</td>\n",
       "      <td>50.88400</td>\n",
       "      <td>1.003100e+00</td>\n",
       "      <td>2.49830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>5.542800</td>\n",
       "      <td>5.39070</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.90943</td>\n",
       "      <td>22.04600</td>\n",
       "      <td>3.793200e+00</td>\n",
       "      <td>3.30760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2.868400</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.688400e+00</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2.568000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.954030</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>3.597200e+00</td>\n",
       "      <td>0.99980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.599400</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.461900e+00</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.006747</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.998600e-01</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.093000</td>\n",
       "      <td>10.97900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.86490</td>\n",
       "      <td>59.29000</td>\n",
       "      <td>3.842400e+00</td>\n",
       "      <td>2.95980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>10.771000</td>\n",
       "      <td>17.35500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.81112</td>\n",
       "      <td>222.77000</td>\n",
       "      <td>3.805600e+00</td>\n",
       "      <td>2.55970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>17.007000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.67070</td>\n",
       "      <td>0.67070</td>\n",
       "      <td>3.863800e+00</td>\n",
       "      <td>0.44975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.002631</td>\n",
       "      <td>7.86870</td>\n",
       "      <td>0.652080</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>41.18900</td>\n",
       "      <td>4.251900e-01</td>\n",
       "      <td>3.81600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.377600</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.382400e+00</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>3.999600</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.886100e+00</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>13.098000</td>\n",
       "      <td>0.99795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.68404</td>\n",
       "      <td>2.54080</td>\n",
       "      <td>3.939900e+00</td>\n",
       "      <td>1.61830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.002485</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.999800e-01</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>5.430500</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.95111</td>\n",
       "      <td>0.95111</td>\n",
       "      <td>3.960100e+00</td>\n",
       "      <td>0.90442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1.319000</td>\n",
       "      <td>0.99999</td>\n",
       "      <td>0.925950</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>3.97490</td>\n",
       "      <td>2.975200e+00</td>\n",
       "      <td>3.94950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>1.005800</td>\n",
       "      <td>0.98592</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.67186</td>\n",
       "      <td>2.22550</td>\n",
       "      <td>1.028700e+00</td>\n",
       "      <td>1.26230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.003265</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.936920</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>8.778200e-01</td>\n",
       "      <td>0.99980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             1         2         3        4          6            11       12\n",
       "0     1.110500   7.42070  0.333350  0.92052   34.05800  9.605900e-01  3.32300\n",
       "1     3.057200   6.67440  1.000000  0.68579   22.46400  3.963300e+00  1.88090\n",
       "2     0.972200   0.00000  0.893130  1.00000    1.00000  1.968900e+00  0.99980\n",
       "3    10.133000   0.00000  1.000000  0.88022    0.88022  3.092900e+00  0.77464\n",
       "4     7.876400   1.00000  1.000000  0.00000    0.00000  3.860700e+00  0.00000\n",
       "5    27.023000  11.85400  1.000000  0.81479   49.62100  3.472000e+00  2.14100\n",
       "6     2.920300   8.99910  0.935850  1.00000   60.99300  3.276100e+00  3.91360\n",
       "7     4.058100   0.99901  1.000000  0.73900    2.80710  3.663600e+00  1.97240\n",
       "8    10.077000   0.00000  0.975760  1.00000    1.00000  3.624900e+00  0.99980\n",
       "9     7.106000  10.95100  1.000000  0.85632   55.02500  3.882700e+00  2.83960\n",
       "10    0.000000   1.00000  1.000000  0.00000    0.00000  9.998000e-01  0.00000\n",
       "11    1.118500   1.00000  1.000000  0.00000    0.00000  3.461500e+00  0.00000\n",
       "12    8.444900   5.38060  0.959190  1.00000   22.73400  3.529000e+00  3.87770\n",
       "13    0.000000   0.99988  1.000000  0.89923    3.53180  9.998000e-01  3.11840\n",
       "14   12.863000   1.00000  1.000000  0.00000    0.00000  3.718000e+00  0.00000\n",
       "15   10.470000  10.99600  1.000000  0.90250   59.98500  3.881300e+00  2.66790\n",
       "16    6.993200   1.00000  1.000000  0.00000    0.00000  3.223500e+00  0.00000\n",
       "17    4.845000   0.00000  1.000000  0.82528    0.82528  3.808500e+00  0.68095\n",
       "18    2.548700   0.00000  1.000000  0.65435    0.65435  3.433000e+00  0.42808\n",
       "19    0.993220   1.00000  1.000000  0.00000    0.00000  3.096500e+00  0.00000\n",
       "20    0.008602  13.68600  1.000000  0.98966  147.25000  9.999400e-01  3.87670\n",
       "21   17.282000  15.98000  1.000000  0.91159  183.13000  3.959500e+00  3.12030\n",
       "22    2.399300   5.16170  1.000000  0.81000   13.28200  3.583300e+00  2.49760\n",
       "23    4.544700   0.00000  1.000000  0.90774    0.90774  3.639900e+00  0.82383\n",
       "24    1.849500   0.00000  0.782150  1.00000    1.00000  2.402700e+00  0.99980\n",
       "25    0.026105   5.36310  1.000000  0.85403   16.89500  1.000300e+00  2.78930\n",
       "26   16.031000  10.02900  1.000000  0.60236   39.01600  3.940300e+00  1.26850\n",
       "27   15.537000   3.84130  0.902500  1.00000    9.00000  3.257400e+00  2.87070\n",
       "28    0.999110   1.00000  1.000000  0.00000    0.00000  3.630100e+00  0.00000\n",
       "29    0.000000   1.00000  1.000000  0.00000    0.00000  9.998000e-01  0.00000\n",
       "..         ...       ...       ...      ...        ...           ...      ...\n",
       "128   0.000000   0.99958  0.917730  1.00000    3.86670  8.420600e-01  3.73950\n",
       "129   9.770900   1.00000  1.000000  0.00000    0.00000  3.622800e+00  0.00000\n",
       "130   1.570800   1.00000  1.000000  0.00000    0.00000  3.832700e+00  0.00000\n",
       "131  14.546000   6.68100  1.000000  0.82129   28.44400  3.876400e+00  2.66690\n",
       "132  14.480000   1.00000  1.000000  0.00000    0.00000  3.920800e+00  0.00000\n",
       "133  10.293000   7.87700  1.000000  0.74799   32.95400  3.871800e+00  2.15520\n",
       "134   1.251300   1.00000  1.000000  0.00000    0.00000  3.747100e+00  0.00000\n",
       "135   9.458800  11.91100  0.886400  1.00000   96.83400  2.777700e+00  3.91540\n",
       "136   0.014189   5.38410  1.000000  0.71740   16.52900  1.000300e+00  2.05820\n",
       "137   0.000000   0.00000  0.000036  1.00000    1.00000  1.320000e-09  0.99980\n",
       "138   0.147290  10.00800  1.000000  0.81668   50.88400  1.003100e+00  2.49830\n",
       "139   5.542800   5.39070  1.000000  0.90943   22.04600  3.793200e+00  3.30760\n",
       "140   2.868400   1.00000  1.000000  0.00000    0.00000  3.688400e+00  0.00000\n",
       "141   2.568000   0.00000  0.954030  1.00000    1.00000  3.597200e+00  0.99980\n",
       "142   5.599400   1.00000  1.000000  0.00000    0.00000  3.461900e+00  0.00000\n",
       "143   0.006747   1.00000  1.000000  0.00000    0.00000  9.998600e-01  0.00000\n",
       "144   6.093000  10.97900  1.000000  0.86490   59.29000  3.842400e+00  2.95980\n",
       "145  10.771000  17.35500  1.000000  0.81112  222.77000  3.805600e+00  2.55970\n",
       "146   0.000000   1.00000  1.000000  0.00000    0.00000  9.998000e-01  0.00000\n",
       "147  17.007000   0.00000  1.000000  0.67070    0.67070  3.863800e+00  0.44975\n",
       "148   0.002631   7.86870  0.652080  1.00000   41.18900  4.251900e-01  3.81600\n",
       "149   5.377600   1.00000  1.000000  0.00000    0.00000  3.382400e+00  0.00000\n",
       "150   3.999600   1.00000  1.000000  0.00000    0.00000  3.886100e+00  0.00000\n",
       "151  13.098000   0.99795  1.000000  0.68404    2.54080  3.939900e+00  1.61830\n",
       "152   0.002485   1.00000  1.000000  0.00000    0.00000  9.999800e-01  0.00000\n",
       "153   5.430500   0.00000  1.000000  0.95111    0.95111  3.960100e+00  0.90442\n",
       "154   0.000000   1.00000  1.000000  0.00000    0.00000  9.998000e-01  0.00000\n",
       "155   1.319000   0.99999  0.925950  1.00000    3.97490  2.975200e+00  3.94950\n",
       "156   1.005800   0.98592  1.000000  0.67186    2.22550  1.028700e+00  1.26230\n",
       "157   0.003265   0.00000  0.936920  1.00000    1.00000  8.778200e-01  0.99980\n",
       "\n",
       "[158 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>468</th>\n",
       "      <th>469</th>\n",
       "      <th>470</th>\n",
       "      <th>471</th>\n",
       "      <th>472</th>\n",
       "      <th>473</th>\n",
       "      <th>474</th>\n",
       "      <th>475</th>\n",
       "      <th>476</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.110500</td>\n",
       "      <td>7.42070</td>\n",
       "      <td>0.333350</td>\n",
       "      <td>0.92052</td>\n",
       "      <td>1.181900</td>\n",
       "      <td>34.05800</td>\n",
       "      <td>0.565950</td>\n",
       "      <td>5.01170</td>\n",
       "      <td>3.332700e-01</td>\n",
       "      <td>0.84787</td>\n",
       "      <td>...</td>\n",
       "      <td>78.0340</td>\n",
       "      <td>9.111200e-03</td>\n",
       "      <td>0.260820</td>\n",
       "      <td>7.530000e-05</td>\n",
       "      <td>0.001987</td>\n",
       "      <td>3.167900e-03</td>\n",
       "      <td>0.092754</td>\n",
       "      <td>2.670000e-05</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.057200</td>\n",
       "      <td>6.67440</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.68579</td>\n",
       "      <td>7.505300</td>\n",
       "      <td>22.46400</td>\n",
       "      <td>2.436400</td>\n",
       "      <td>3.75160</td>\n",
       "      <td>1.002600e+00</td>\n",
       "      <td>0.47021</td>\n",
       "      <td>...</td>\n",
       "      <td>55.8380</td>\n",
       "      <td>8.488000e-02</td>\n",
       "      <td>0.185620</td>\n",
       "      <td>6.618600e-04</td>\n",
       "      <td>0.001335</td>\n",
       "      <td>2.433800e-02</td>\n",
       "      <td>0.065884</td>\n",
       "      <td>1.948000e-04</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.972200</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.893130</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.753400</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.403300</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>7.975200e-01</td>\n",
       "      <td>0.99980</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>2.331000e-02</td>\n",
       "      <td>0.006296</td>\n",
       "      <td>1.752200e-04</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>8.180200e-03</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>6.280000e-05</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.133000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.88022</td>\n",
       "      <td>57.266000</td>\n",
       "      <td>0.88022</td>\n",
       "      <td>6.382300</td>\n",
       "      <td>0.88022</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.77464</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>5.836800e-01</td>\n",
       "      <td>0.006296</td>\n",
       "      <td>4.646000e-03</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>1.929600e-01</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>1.569300e-03</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.876400</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>44.418000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6.353000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.074500e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.524000e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.096200e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.169500e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27.023000</td>\n",
       "      <td>11.85400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.81479</td>\n",
       "      <td>477.400000</td>\n",
       "      <td>49.62100</td>\n",
       "      <td>14.479000</td>\n",
       "      <td>4.82180</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.66374</td>\n",
       "      <td>...</td>\n",
       "      <td>213.7800</td>\n",
       "      <td>7.269700e+00</td>\n",
       "      <td>0.570400</td>\n",
       "      <td>5.286300e-02</td>\n",
       "      <td>0.004118</td>\n",
       "      <td>2.580000e+00</td>\n",
       "      <td>0.223090</td>\n",
       "      <td>1.919900e-02</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.920300</td>\n",
       "      <td>8.99910</td>\n",
       "      <td>0.935850</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>7.894900</td>\n",
       "      <td>60.99300</td>\n",
       "      <td>2.556800</td>\n",
       "      <td>7.62570</td>\n",
       "      <td>8.761700e-01</td>\n",
       "      <td>0.99980</td>\n",
       "      <td>...</td>\n",
       "      <td>125.1900</td>\n",
       "      <td>6.002000e-02</td>\n",
       "      <td>0.401060</td>\n",
       "      <td>4.862900e-04</td>\n",
       "      <td>0.003301</td>\n",
       "      <td>2.104900e-02</td>\n",
       "      <td>0.143010</td>\n",
       "      <td>1.739900e-04</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.058100</td>\n",
       "      <td>0.99901</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.73900</td>\n",
       "      <td>13.638000</td>\n",
       "      <td>2.80710</td>\n",
       "      <td>3.416100</td>\n",
       "      <td>1.40450</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.54600</td>\n",
       "      <td>...</td>\n",
       "      <td>7.2532</td>\n",
       "      <td>9.299200e-02</td>\n",
       "      <td>0.018933</td>\n",
       "      <td>7.244600e-04</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>3.309100e-02</td>\n",
       "      <td>0.007370</td>\n",
       "      <td>2.631600e-04</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.077000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.975760</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>67.060000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>7.459400</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>9.520700e-01</td>\n",
       "      <td>0.99980</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>5.432100e-01</td>\n",
       "      <td>0.006296</td>\n",
       "      <td>4.315200e-03</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>1.864200e-01</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>1.511800e-03</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.106000</td>\n",
       "      <td>10.95100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.85632</td>\n",
       "      <td>26.669000</td>\n",
       "      <td>55.02500</td>\n",
       "      <td>4.428200</td>\n",
       "      <td>5.60190</td>\n",
       "      <td>1.000200e+00</td>\n",
       "      <td>0.73313</td>\n",
       "      <td>...</td>\n",
       "      <td>157.4900</td>\n",
       "      <td>4.121800e-01</td>\n",
       "      <td>0.306160</td>\n",
       "      <td>3.346300e-03</td>\n",
       "      <td>0.002097</td>\n",
       "      <td>1.094900e-01</td>\n",
       "      <td>0.133660</td>\n",
       "      <td>9.129700e-04</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.004400e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.650000e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000100e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.640000e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.118500</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.766700</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.860600</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.493500e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.990300e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.878800e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.230000e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8.444900</td>\n",
       "      <td>5.38060</td>\n",
       "      <td>0.959190</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>43.805000</td>\n",
       "      <td>22.73400</td>\n",
       "      <td>6.090700</td>\n",
       "      <td>4.56100</td>\n",
       "      <td>9.203900e-01</td>\n",
       "      <td>0.99980</td>\n",
       "      <td>...</td>\n",
       "      <td>45.3160</td>\n",
       "      <td>3.119300e-01</td>\n",
       "      <td>0.138390</td>\n",
       "      <td>2.495200e-03</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>1.110600e-01</td>\n",
       "      <td>0.050908</td>\n",
       "      <td>9.063900e-04</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.99988</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.89923</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.53180</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.76610</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.80844</td>\n",
       "      <td>...</td>\n",
       "      <td>7.9394</td>\n",
       "      <td>5.854800e-03</td>\n",
       "      <td>0.025183</td>\n",
       "      <td>4.450000e-05</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>2.093600e-03</td>\n",
       "      <td>0.008983</td>\n",
       "      <td>1.620000e-05</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12.863000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>114.040000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.544900</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.206900e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.326200e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.212100e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.608000e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10.470000</td>\n",
       "      <td>10.99600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.90250</td>\n",
       "      <td>56.776000</td>\n",
       "      <td>59.98500</td>\n",
       "      <td>6.274300</td>\n",
       "      <td>6.05000</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.81434</td>\n",
       "      <td>...</td>\n",
       "      <td>169.9900</td>\n",
       "      <td>4.937800e-01</td>\n",
       "      <td>0.509820</td>\n",
       "      <td>3.710800e-03</td>\n",
       "      <td>0.003802</td>\n",
       "      <td>1.705800e-01</td>\n",
       "      <td>0.189260</td>\n",
       "      <td>1.311400e-03</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6.993200</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>26.041000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.288900</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.320600e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.801100e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.976200e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.324600e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.845000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.82528</td>\n",
       "      <td>15.664000</td>\n",
       "      <td>0.82528</td>\n",
       "      <td>3.616900</td>\n",
       "      <td>0.82528</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.68095</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8404</td>\n",
       "      <td>1.034700e-01</td>\n",
       "      <td>0.003784</td>\n",
       "      <td>8.352500e-04</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>3.685000e-02</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>3.034200e-04</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.548700</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.65435</td>\n",
       "      <td>7.259100</td>\n",
       "      <td>0.65435</td>\n",
       "      <td>2.437200</td>\n",
       "      <td>0.65435</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.42808</td>\n",
       "      <td>...</td>\n",
       "      <td>1.7002</td>\n",
       "      <td>5.394400e-02</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>4.249400e-04</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>1.920200e-02</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>1.543600e-04</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.993220</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.503100</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.759800</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.517700e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.088900e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.980400e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.590000e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.008602</td>\n",
       "      <td>13.68600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.98966</td>\n",
       "      <td>1.020800</td>\n",
       "      <td>147.25000</td>\n",
       "      <td>1.000100</td>\n",
       "      <td>11.34300</td>\n",
       "      <td>9.999400e-01</td>\n",
       "      <td>0.97924</td>\n",
       "      <td>...</td>\n",
       "      <td>319.7600</td>\n",
       "      <td>8.944800e-03</td>\n",
       "      <td>1.047700</td>\n",
       "      <td>7.270000e-05</td>\n",
       "      <td>0.008507</td>\n",
       "      <td>2.831900e-03</td>\n",
       "      <td>0.373420</td>\n",
       "      <td>2.350000e-05</td>\n",
       "      <td>0.003091</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>17.282000</td>\n",
       "      <td>15.98000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.91159</td>\n",
       "      <td>235.210000</td>\n",
       "      <td>183.13000</td>\n",
       "      <td>13.653000</td>\n",
       "      <td>11.48600</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.83083</td>\n",
       "      <td>...</td>\n",
       "      <td>420.9700</td>\n",
       "      <td>1.604700e+00</td>\n",
       "      <td>0.961180</td>\n",
       "      <td>1.187500e-02</td>\n",
       "      <td>0.006862</td>\n",
       "      <td>5.699400e-01</td>\n",
       "      <td>0.395870</td>\n",
       "      <td>4.313100e-03</td>\n",
       "      <td>0.002882</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.399300</td>\n",
       "      <td>5.16170</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.81000</td>\n",
       "      <td>4.694400</td>\n",
       "      <td>13.28200</td>\n",
       "      <td>1.910200</td>\n",
       "      <td>2.82810</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.65597</td>\n",
       "      <td>...</td>\n",
       "      <td>39.4140</td>\n",
       "      <td>3.661000e-02</td>\n",
       "      <td>0.134690</td>\n",
       "      <td>2.903500e-04</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>1.158300e-02</td>\n",
       "      <td>0.047861</td>\n",
       "      <td>9.410000e-05</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.544700</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.90774</td>\n",
       "      <td>13.989000</td>\n",
       "      <td>0.90774</td>\n",
       "      <td>3.400400</td>\n",
       "      <td>0.90774</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.82383</td>\n",
       "      <td>...</td>\n",
       "      <td>1.9703</td>\n",
       "      <td>9.952700e-02</td>\n",
       "      <td>0.005781</td>\n",
       "      <td>7.925900e-04</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>3.543600e-02</td>\n",
       "      <td>0.002126</td>\n",
       "      <td>2.879100e-04</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.849500</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.782150</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>3.507500</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.550300</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>6.214700e-01</td>\n",
       "      <td>0.99980</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>3.062000e-02</td>\n",
       "      <td>0.006296</td>\n",
       "      <td>2.511600e-04</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>1.034900e-02</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>8.670000e-05</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.026105</td>\n",
       "      <td>5.36310</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.85403</td>\n",
       "      <td>1.041800</td>\n",
       "      <td>16.89500</td>\n",
       "      <td>1.000200</td>\n",
       "      <td>3.40780</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.72922</td>\n",
       "      <td>...</td>\n",
       "      <td>43.8210</td>\n",
       "      <td>7.314700e-03</td>\n",
       "      <td>0.098191</td>\n",
       "      <td>6.100000e-05</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>2.604200e-03</td>\n",
       "      <td>0.040561</td>\n",
       "      <td>2.220000e-05</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>16.031000</td>\n",
       "      <td>10.02900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.60236</td>\n",
       "      <td>171.920000</td>\n",
       "      <td>39.01600</td>\n",
       "      <td>10.983000</td>\n",
       "      <td>4.35300</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.36277</td>\n",
       "      <td>...</td>\n",
       "      <td>116.1200</td>\n",
       "      <td>1.214200e+00</td>\n",
       "      <td>0.155290</td>\n",
       "      <td>8.531700e-03</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>4.306000e-01</td>\n",
       "      <td>0.077713</td>\n",
       "      <td>3.098300e-03</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>15.537000</td>\n",
       "      <td>3.84130</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>146.810000</td>\n",
       "      <td>9.00000</td>\n",
       "      <td>9.783500</td>\n",
       "      <td>2.37630</td>\n",
       "      <td>8.143400e-01</td>\n",
       "      <td>0.99980</td>\n",
       "      <td>...</td>\n",
       "      <td>27.5350</td>\n",
       "      <td>1.305700e+00</td>\n",
       "      <td>0.081373</td>\n",
       "      <td>9.967200e-03</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>4.643200e-01</td>\n",
       "      <td>0.030451</td>\n",
       "      <td>3.620400e-03</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.999110</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.808400</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.905400</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.222800e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.710500e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.044700e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.960000e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.470000e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.420000e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.307700e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.970000e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.99958</td>\n",
       "      <td>0.917730</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.917730</td>\n",
       "      <td>3.86670</td>\n",
       "      <td>0.917730</td>\n",
       "      <td>1.93390</td>\n",
       "      <td>8.420600e-01</td>\n",
       "      <td>0.99980</td>\n",
       "      <td>...</td>\n",
       "      <td>7.7576</td>\n",
       "      <td>9.910900e-03</td>\n",
       "      <td>0.024937</td>\n",
       "      <td>8.400000e-05</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>2.951400e-03</td>\n",
       "      <td>0.008890</td>\n",
       "      <td>2.560000e-05</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>9.770900</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>49.556000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.998900</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.481500e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.471000e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.481500e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.174100e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1.570800</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.211700</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.958000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.003900e+00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.640500e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.195200e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.413600e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.970000e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>14.546000</td>\n",
       "      <td>6.68100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.82129</td>\n",
       "      <td>143.130000</td>\n",
       "      <td>28.44400</td>\n",
       "      <td>10.268000</td>\n",
       "      <td>4.74210</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.67438</td>\n",
       "      <td>...</td>\n",
       "      <td>62.5650</td>\n",
       "      <td>1.188100e+00</td>\n",
       "      <td>0.107240</td>\n",
       "      <td>9.061200e-03</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>4.141800e-01</td>\n",
       "      <td>0.048749</td>\n",
       "      <td>3.228100e-03</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>14.480000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>159.440000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>11.422000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.207500e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.733500e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.302900e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.536000e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>10.293000</td>\n",
       "      <td>7.87700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.74799</td>\n",
       "      <td>56.453000</td>\n",
       "      <td>32.95400</td>\n",
       "      <td>6.288600</td>\n",
       "      <td>4.71260</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.55938</td>\n",
       "      <td>...</td>\n",
       "      <td>86.9710</td>\n",
       "      <td>5.351200e-01</td>\n",
       "      <td>0.210110</td>\n",
       "      <td>4.166100e-03</td>\n",
       "      <td>0.001581</td>\n",
       "      <td>1.820900e-01</td>\n",
       "      <td>0.084363</td>\n",
       "      <td>1.448600e-03</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1.251300</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.936400</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.258500e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.728100e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.048900e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.960000e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>9.458800</td>\n",
       "      <td>11.91100</td>\n",
       "      <td>0.886400</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>43.574000</td>\n",
       "      <td>96.83400</td>\n",
       "      <td>5.363600</td>\n",
       "      <td>8.90190</td>\n",
       "      <td>7.867600e-01</td>\n",
       "      <td>0.99980</td>\n",
       "      <td>...</td>\n",
       "      <td>200.8400</td>\n",
       "      <td>3.398700e-01</td>\n",
       "      <td>0.357990</td>\n",
       "      <td>2.457800e-03</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>1.206300e-01</td>\n",
       "      <td>0.161090</td>\n",
       "      <td>8.926100e-04</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.014189</td>\n",
       "      <td>5.38410</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.71740</td>\n",
       "      <td>1.033100</td>\n",
       "      <td>16.52900</td>\n",
       "      <td>1.000300</td>\n",
       "      <td>3.31280</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.51456</td>\n",
       "      <td>...</td>\n",
       "      <td>43.7680</td>\n",
       "      <td>6.295900e-03</td>\n",
       "      <td>0.084053</td>\n",
       "      <td>5.220000e-05</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>2.245600e-03</td>\n",
       "      <td>0.036691</td>\n",
       "      <td>1.900000e-05</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.320000e-09</td>\n",
       "      <td>0.99980</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1.710000e-14</td>\n",
       "      <td>0.006296</td>\n",
       "      <td>6.670000e-17</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>2.780000e-12</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>1.090000e-14</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.147290</td>\n",
       "      <td>10.00800</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.81668</td>\n",
       "      <td>1.122000</td>\n",
       "      <td>50.88400</td>\n",
       "      <td>1.001600</td>\n",
       "      <td>5.70380</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.66683</td>\n",
       "      <td>...</td>\n",
       "      <td>144.2300</td>\n",
       "      <td>8.268000e-03</td>\n",
       "      <td>0.468850</td>\n",
       "      <td>7.030000e-05</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>2.792000e-03</td>\n",
       "      <td>0.168750</td>\n",
       "      <td>2.420000e-05</td>\n",
       "      <td>0.001341</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>5.542800</td>\n",
       "      <td>5.39070</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.90943</td>\n",
       "      <td>20.459000</td>\n",
       "      <td>22.04600</td>\n",
       "      <td>4.084900</td>\n",
       "      <td>4.41020</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.82690</td>\n",
       "      <td>...</td>\n",
       "      <td>46.6470</td>\n",
       "      <td>1.460700e-01</td>\n",
       "      <td>0.130900</td>\n",
       "      <td>1.132700e-03</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>5.196800e-02</td>\n",
       "      <td>0.049448</td>\n",
       "      <td>4.114400e-04</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2.868400</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>8.518300</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.764600</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000500e+00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.491000e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.156500e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.400700e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.016700e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2.568000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.954030</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>8.383000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.783900</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>9.099900e-01</td>\n",
       "      <td>0.99980</td>\n",
       "      <td>...</td>\n",
       "      <td>1.9940</td>\n",
       "      <td>5.642500e-02</td>\n",
       "      <td>0.006183</td>\n",
       "      <td>4.648300e-04</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>2.012100e-02</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>1.688700e-04</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.599400</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>21.635000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.286600</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.438500e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.033900e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.297700e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.226100e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.006747</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.020700</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000100</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.998600e-01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.337900e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.210600e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.586600e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.320000e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.093000</td>\n",
       "      <td>10.97900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.86490</td>\n",
       "      <td>21.949000</td>\n",
       "      <td>59.29000</td>\n",
       "      <td>4.242300</td>\n",
       "      <td>6.00090</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.74790</td>\n",
       "      <td>...</td>\n",
       "      <td>165.5000</td>\n",
       "      <td>1.672100e-01</td>\n",
       "      <td>0.411940</td>\n",
       "      <td>1.318700e-03</td>\n",
       "      <td>0.002990</td>\n",
       "      <td>5.945800e-02</td>\n",
       "      <td>0.164510</td>\n",
       "      <td>4.789900e-04</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>10.771000</td>\n",
       "      <td>17.35500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.81112</td>\n",
       "      <td>66.104000</td>\n",
       "      <td>222.77000</td>\n",
       "      <td>7.186800</td>\n",
       "      <td>12.44300</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.65778</td>\n",
       "      <td>...</td>\n",
       "      <td>524.9300</td>\n",
       "      <td>4.669600e-01</td>\n",
       "      <td>1.075100</td>\n",
       "      <td>3.546100e-03</td>\n",
       "      <td>0.007501</td>\n",
       "      <td>1.659400e-01</td>\n",
       "      <td>0.460380</td>\n",
       "      <td>1.288000e-03</td>\n",
       "      <td>0.003275</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8.443200e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.300000e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.709900e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.390000e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>17.007000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.67070</td>\n",
       "      <td>199.880000</td>\n",
       "      <td>0.67070</td>\n",
       "      <td>11.801000</td>\n",
       "      <td>0.67070</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.44975</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8592</td>\n",
       "      <td>1.730300e+00</td>\n",
       "      <td>0.004034</td>\n",
       "      <td>1.318000e-02</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>6.041100e-01</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>4.702100e-03</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.002631</td>\n",
       "      <td>7.86870</td>\n",
       "      <td>0.652080</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.659680</td>\n",
       "      <td>41.18900</td>\n",
       "      <td>0.652100</td>\n",
       "      <td>5.90240</td>\n",
       "      <td>4.251900e-01</td>\n",
       "      <td>0.99980</td>\n",
       "      <td>...</td>\n",
       "      <td>90.8080</td>\n",
       "      <td>8.107200e-03</td>\n",
       "      <td>0.299660</td>\n",
       "      <td>6.470000e-05</td>\n",
       "      <td>0.002394</td>\n",
       "      <td>2.664300e-03</td>\n",
       "      <td>0.106750</td>\n",
       "      <td>2.170000e-05</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.377600</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>20.047000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.026200</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.568100e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.292700e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.592000e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.696200e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>3.999600</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>15.430000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>3.858000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9.831500e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.900700e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.502900e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.870100e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>13.098000</td>\n",
       "      <td>0.99795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.68404</td>\n",
       "      <td>104.560000</td>\n",
       "      <td>2.54080</td>\n",
       "      <td>8.693100</td>\n",
       "      <td>1.27220</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.46782</td>\n",
       "      <td>...</td>\n",
       "      <td>6.7034</td>\n",
       "      <td>8.683800e-01</td>\n",
       "      <td>0.010928</td>\n",
       "      <td>6.747500e-03</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>3.089700e-01</td>\n",
       "      <td>0.005073</td>\n",
       "      <td>2.451000e-03</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.002485</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.011300</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.999800e-01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.295900e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.220000e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.245600e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.900000e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>5.430500</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.95111</td>\n",
       "      <td>23.971000</td>\n",
       "      <td>0.95111</td>\n",
       "      <td>4.789500</td>\n",
       "      <td>0.95111</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.90442</td>\n",
       "      <td>...</td>\n",
       "      <td>1.9140</td>\n",
       "      <td>1.545000e-01</td>\n",
       "      <td>0.004837</td>\n",
       "      <td>1.241200e-03</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>5.504300e-02</td>\n",
       "      <td>0.001886</td>\n",
       "      <td>4.508800e-04</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.332900e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.310000e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.259600e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.930000e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>1.319000</td>\n",
       "      <td>0.99999</td>\n",
       "      <td>0.925950</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>3.607600</td>\n",
       "      <td>3.97490</td>\n",
       "      <td>1.725000</td>\n",
       "      <td>1.98750</td>\n",
       "      <td>8.598600e-01</td>\n",
       "      <td>0.99980</td>\n",
       "      <td>...</td>\n",
       "      <td>7.9394</td>\n",
       "      <td>2.989700e-02</td>\n",
       "      <td>0.025183</td>\n",
       "      <td>2.406400e-04</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>9.879200e-03</td>\n",
       "      <td>0.008983</td>\n",
       "      <td>8.120000e-05</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>1.005800</td>\n",
       "      <td>0.98592</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.67186</td>\n",
       "      <td>1.471600</td>\n",
       "      <td>2.22550</td>\n",
       "      <td>1.014800</td>\n",
       "      <td>1.12360</td>\n",
       "      <td>9.998000e-01</td>\n",
       "      <td>0.45131</td>\n",
       "      <td>...</td>\n",
       "      <td>6.8659</td>\n",
       "      <td>7.560300e-03</td>\n",
       "      <td>0.012189</td>\n",
       "      <td>6.240000e-05</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>2.691300e-03</td>\n",
       "      <td>0.005478</td>\n",
       "      <td>2.270000e-05</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.003265</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.936920</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.949380</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.936970</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>8.778200e-01</td>\n",
       "      <td>0.99980</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>6.637200e-03</td>\n",
       "      <td>0.006296</td>\n",
       "      <td>5.070000e-05</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>2.252400e-03</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>1.760000e-05</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158 rows Ã— 477 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             1         2         3        4           5          6          7  \\\n",
       "0     1.110500   7.42070  0.333350  0.92052    1.181900   34.05800   0.565950   \n",
       "1     3.057200   6.67440  1.000000  0.68579    7.505300   22.46400   2.436400   \n",
       "2     0.972200   0.00000  0.893130  1.00000    2.753400    1.00000   1.403300   \n",
       "3    10.133000   0.00000  1.000000  0.88022   57.266000    0.88022   6.382300   \n",
       "4     7.876400   1.00000  1.000000  0.00000   44.418000    0.00000   6.353000   \n",
       "5    27.023000  11.85400  1.000000  0.81479  477.400000   49.62100  14.479000   \n",
       "6     2.920300   8.99910  0.935850  1.00000    7.894900   60.99300   2.556800   \n",
       "7     4.058100   0.99901  1.000000  0.73900   13.638000    2.80710   3.416100   \n",
       "8    10.077000   0.00000  0.975760  1.00000   67.060000    1.00000   7.459400   \n",
       "9     7.106000  10.95100  1.000000  0.85632   26.669000   55.02500   4.428200   \n",
       "10    0.000000   1.00000  1.000000  0.00000    1.000000    0.00000   1.000000   \n",
       "11    1.118500   1.00000  1.000000  0.00000    3.766700    0.00000   1.860600   \n",
       "12    8.444900   5.38060  0.959190  1.00000   43.805000   22.73400   6.090700   \n",
       "13    0.000000   0.99988  1.000000  0.89923    1.000000    3.53180   1.000000   \n",
       "14   12.863000   1.00000  1.000000  0.00000  114.040000    0.00000   9.544900   \n",
       "15   10.470000  10.99600  1.000000  0.90250   56.776000   59.98500   6.274300   \n",
       "16    6.993200   1.00000  1.000000  0.00000   26.041000    0.00000   4.288900   \n",
       "17    4.845000   0.00000  1.000000  0.82528   15.664000    0.82528   3.616900   \n",
       "18    2.548700   0.00000  1.000000  0.65435    7.259100    0.65435   2.437200   \n",
       "19    0.993220   1.00000  1.000000  0.00000    3.503100    0.00000   1.759800   \n",
       "20    0.008602  13.68600  1.000000  0.98966    1.020800  147.25000   1.000100   \n",
       "21   17.282000  15.98000  1.000000  0.91159  235.210000  183.13000  13.653000   \n",
       "22    2.399300   5.16170  1.000000  0.81000    4.694400   13.28200   1.910200   \n",
       "23    4.544700   0.00000  1.000000  0.90774   13.989000    0.90774   3.400400   \n",
       "24    1.849500   0.00000  0.782150  1.00000    3.507500    1.00000   1.550300   \n",
       "25    0.026105   5.36310  1.000000  0.85403    1.041800   16.89500   1.000200   \n",
       "26   16.031000  10.02900  1.000000  0.60236  171.920000   39.01600  10.983000   \n",
       "27   15.537000   3.84130  0.902500  1.00000  146.810000    9.00000   9.783500   \n",
       "28    0.999110   1.00000  1.000000  0.00000    3.808400    0.00000   1.905400   \n",
       "29    0.000000   1.00000  1.000000  0.00000    1.000000    0.00000   1.000000   \n",
       "..         ...       ...       ...      ...         ...        ...        ...   \n",
       "128   0.000000   0.99958  0.917730  1.00000    0.917730    3.86670   0.917730   \n",
       "129   9.770900   1.00000  1.000000  0.00000   49.556000    0.00000   5.998900   \n",
       "130   1.570800   1.00000  1.000000  0.00000    4.211700    0.00000   1.958000   \n",
       "131  14.546000   6.68100  1.000000  0.82129  143.130000   28.44400  10.268000   \n",
       "132  14.480000   1.00000  1.000000  0.00000  159.440000    0.00000  11.422000   \n",
       "133  10.293000   7.87700  1.000000  0.74799   56.453000   32.95400   6.288600   \n",
       "134   1.251300   1.00000  1.000000  0.00000    4.000000    0.00000   1.936400   \n",
       "135   9.458800  11.91100  0.886400  1.00000   43.574000   96.83400   5.363600   \n",
       "136   0.014189   5.38410  1.000000  0.71740    1.033100   16.52900   1.000300   \n",
       "137   0.000000   0.00000  0.000036  1.00000    0.000036    1.00000   0.000036   \n",
       "138   0.147290  10.00800  1.000000  0.81668    1.122000   50.88400   1.001600   \n",
       "139   5.542800   5.39070  1.000000  0.90943   20.459000   22.04600   4.084900   \n",
       "140   2.868400   1.00000  1.000000  0.00000    8.518300    0.00000   2.764600   \n",
       "141   2.568000   0.00000  0.954030  1.00000    8.383000    1.00000   2.783900   \n",
       "142   5.599400   1.00000  1.000000  0.00000   21.635000    0.00000   4.286600   \n",
       "143   0.006747   1.00000  1.000000  0.00000    1.020700    0.00000   1.000100   \n",
       "144   6.093000  10.97900  1.000000  0.86490   21.949000   59.29000   4.242300   \n",
       "145  10.771000  17.35500  1.000000  0.81112   66.104000  222.77000   7.186800   \n",
       "146   0.000000   1.00000  1.000000  0.00000    1.000000    0.00000   1.000000   \n",
       "147  17.007000   0.00000  1.000000  0.67070  199.880000    0.67070  11.801000   \n",
       "148   0.002631   7.86870  0.652080  1.00000    0.659680   41.18900   0.652100   \n",
       "149   5.377600   1.00000  1.000000  0.00000   20.047000    0.00000   4.026200   \n",
       "150   3.999600   1.00000  1.000000  0.00000   15.430000    0.00000   3.858000   \n",
       "151  13.098000   0.99795  1.000000  0.68404  104.560000    2.54080   8.693100   \n",
       "152   0.002485   1.00000  1.000000  0.00000    1.011300    0.00000   1.000000   \n",
       "153   5.430500   0.00000  1.000000  0.95111   23.971000    0.95111   4.789500   \n",
       "154   0.000000   1.00000  1.000000  0.00000    1.000000    0.00000   1.000000   \n",
       "155   1.319000   0.99999  0.925950  1.00000    3.607600    3.97490   1.725000   \n",
       "156   1.005800   0.98592  1.000000  0.67186    1.471600    2.22550   1.014800   \n",
       "157   0.003265   0.00000  0.936920  1.00000    0.949380    1.00000   0.936970   \n",
       "\n",
       "            8             9       10  ...         468           469       470  \\\n",
       "0     5.01170  3.332700e-01  0.84787  ...     78.0340  9.111200e-03  0.260820   \n",
       "1     3.75160  1.002600e+00  0.47021  ...     55.8380  8.488000e-02  0.185620   \n",
       "2     1.00000  7.975200e-01  0.99980  ...      2.0000  2.331000e-02  0.006296   \n",
       "3     0.88022  9.998000e-01  0.77464  ...      2.0000  5.836800e-01  0.006296   \n",
       "4     0.00000  9.998000e-01  0.00000  ...      0.0000  3.074500e-01  0.000000   \n",
       "5     4.82180  9.998000e-01  0.66374  ...    213.7800  7.269700e+00  0.570400   \n",
       "6     7.62570  8.761700e-01  0.99980  ...    125.1900  6.002000e-02  0.401060   \n",
       "7     1.40450  9.998000e-01  0.54600  ...      7.2532  9.299200e-02  0.018933   \n",
       "8     1.00000  9.520700e-01  0.99980  ...      2.0000  5.432100e-01  0.006296   \n",
       "9     5.60190  1.000200e+00  0.73313  ...    157.4900  4.121800e-01  0.306160   \n",
       "10    0.00000  9.998000e-01  0.00000  ...      0.0000  1.004400e-02  0.000000   \n",
       "11    0.00000  9.998000e-01  0.00000  ...      0.0000  2.493500e-02  0.000000   \n",
       "12    4.56100  9.203900e-01  0.99980  ...     45.3160  3.119300e-01  0.138390   \n",
       "13    1.76610  9.998000e-01  0.80844  ...      7.9394  5.854800e-03  0.025183   \n",
       "14    0.00000  9.998000e-01  0.00000  ...      0.0000  9.206900e-01  0.000000   \n",
       "15    6.05000  9.998000e-01  0.81434  ...    169.9900  4.937800e-01  0.509820   \n",
       "16    0.00000  9.998000e-01  0.00000  ...      0.0000  2.320600e-01  0.000000   \n",
       "17    0.82528  9.998000e-01  0.68095  ...      1.8404  1.034700e-01  0.003784   \n",
       "18    0.65435  9.998000e-01  0.42808  ...      1.7002  5.394400e-02  0.002272   \n",
       "19    0.00000  9.998000e-01  0.00000  ...      0.0000  2.517700e-02  0.000000   \n",
       "20   11.34300  9.999400e-01  0.97924  ...    319.7600  8.944800e-03  1.047700   \n",
       "21   11.48600  9.998000e-01  0.83083  ...    420.9700  1.604700e+00  0.961180   \n",
       "22    2.82810  9.998000e-01  0.65597  ...     39.4140  3.661000e-02  0.134690   \n",
       "23    0.90774  9.998000e-01  0.82383  ...      1.9703  9.952700e-02  0.005781   \n",
       "24    1.00000  6.214700e-01  0.99980  ...      2.0000  3.062000e-02  0.006296   \n",
       "25    3.40780  9.998000e-01  0.72922  ...     43.8210  7.314700e-03  0.098191   \n",
       "26    4.35300  9.998000e-01  0.36277  ...    116.1200  1.214200e+00  0.155290   \n",
       "27    2.37630  8.143400e-01  0.99980  ...     27.5350  1.305700e+00  0.081373   \n",
       "28    0.00000  9.998000e-01  0.00000  ...      0.0000  3.222800e-02  0.000000   \n",
       "29    0.00000  9.998000e-01  0.00000  ...      0.0000  6.470000e-03  0.000000   \n",
       "..        ...           ...      ...  ...         ...           ...       ...   \n",
       "128   1.93390  8.420600e-01  0.99980  ...      7.7576  9.910900e-03  0.024937   \n",
       "129   0.00000  9.998000e-01  0.00000  ...      0.0000  4.481500e-01  0.000000   \n",
       "130   0.00000  1.003900e+00  0.00000  ...      0.0000  2.640500e-02  0.000000   \n",
       "131   4.74210  9.998000e-01  0.67438  ...     62.5650  1.188100e+00  0.107240   \n",
       "132   0.00000  9.998000e-01  0.00000  ...      0.0000  1.207500e+00  0.000000   \n",
       "133   4.71260  9.998000e-01  0.55938  ...     86.9710  5.351200e-01  0.210110   \n",
       "134   0.00000  9.998000e-01  0.00000  ...      0.0000  3.258500e-02  0.000000   \n",
       "135   8.90190  7.867600e-01  0.99980  ...    200.8400  3.398700e-01  0.357990   \n",
       "136   3.31280  9.998000e-01  0.51456  ...     43.7680  6.295900e-03  0.084053   \n",
       "137   1.00000  1.320000e-09  0.99980  ...      2.0000  1.710000e-14  0.006296   \n",
       "138   5.70380  9.998000e-01  0.66683  ...    144.2300  8.268000e-03  0.468850   \n",
       "139   4.41020  9.998000e-01  0.82690  ...     46.6470  1.460700e-01  0.130900   \n",
       "140   0.00000  1.000500e+00  0.00000  ...      0.0000  7.491000e-02  0.000000   \n",
       "141   1.00000  9.099900e-01  0.99980  ...      1.9940  5.642500e-02  0.006183   \n",
       "142   0.00000  9.998000e-01  0.00000  ...      0.0000  2.438500e-01  0.000000   \n",
       "143   0.00000  9.998600e-01  0.00000  ...      0.0000  1.337900e-02  0.000000   \n",
       "144   6.00090  9.998000e-01  0.74790  ...    165.5000  1.672100e-01  0.411940   \n",
       "145  12.44300  9.998000e-01  0.65778  ...    524.9300  4.669600e-01  1.075100   \n",
       "146   0.00000  9.998000e-01  0.00000  ...      0.0000  8.443200e-03  0.000000   \n",
       "147   0.67070  9.998000e-01  0.44975  ...      1.8592  1.730300e+00  0.004034   \n",
       "148   5.90240  4.251900e-01  0.99980  ...     90.8080  8.107200e-03  0.299660   \n",
       "149   0.00000  9.998000e-01  0.00000  ...      0.0000  1.568100e-01  0.000000   \n",
       "150   0.00000  9.998000e-01  0.00000  ...      0.0000  9.831500e-02  0.000000   \n",
       "151   1.27220  9.998000e-01  0.46782  ...      6.7034  8.683800e-01  0.010928   \n",
       "152   0.00000  9.999800e-01  0.00000  ...      0.0000  6.295900e-03  0.000000   \n",
       "153   0.95111  9.998000e-01  0.90442  ...      1.9140  1.545000e-01  0.004837   \n",
       "154   0.00000  9.998000e-01  0.00000  ...      0.0000  6.332900e-03  0.000000   \n",
       "155   1.98750  8.598600e-01  0.99980  ...      7.9394  2.989700e-02  0.025183   \n",
       "156   1.12360  9.998000e-01  0.45131  ...      6.8659  7.560300e-03  0.012189   \n",
       "157   1.00000  8.778200e-01  0.99980  ...      2.0000  6.637200e-03  0.006296   \n",
       "\n",
       "              471       472           473       474           475       476  \\\n",
       "0    7.530000e-05  0.001987  3.167900e-03  0.092754  2.670000e-05  0.000722   \n",
       "1    6.618600e-04  0.001335  2.433800e-02  0.065884  1.948000e-04  0.000485   \n",
       "2    1.752200e-04  0.000052  8.180200e-03  0.002246  6.280000e-05  0.000019   \n",
       "3    4.646000e-03  0.000052  1.929600e-01  0.002246  1.569300e-03  0.000019   \n",
       "4    2.524000e-03  0.000000  1.096200e-01  0.000000  9.169500e-04  0.000000   \n",
       "5    5.286300e-02  0.004118  2.580000e+00  0.223090  1.919900e-02  0.001645   \n",
       "6    4.862900e-04  0.003301  2.104900e-02  0.143010  1.739900e-04  0.001199   \n",
       "7    7.244600e-04  0.000146  3.309100e-02  0.007370  2.631600e-04  0.000058   \n",
       "8    4.315200e-03  0.000052  1.864200e-01  0.002246  1.511800e-03  0.000019   \n",
       "9    3.346300e-03  0.002097  1.094900e-01  0.133660  9.129700e-04  0.000934   \n",
       "10   8.650000e-05  0.000000  3.000100e-03  0.000000  2.640000e-05  0.000000   \n",
       "11   1.990300e-04  0.000000  8.878800e-03  0.000000  7.230000e-05  0.000000   \n",
       "12   2.495200e-03  0.001085  1.110600e-01  0.050908  9.063900e-04  0.000407   \n",
       "13   4.450000e-05  0.000209  2.093600e-03  0.008983  1.620000e-05  0.000076   \n",
       "14   7.326200e-03  0.000000  3.212100e-01  0.000000  2.608000e-03  0.000000   \n",
       "15   3.710800e-03  0.003802  1.705800e-01  0.189260  1.311400e-03  0.001441   \n",
       "16   1.801100e-03  0.000000  7.976200e-02  0.000000  6.324600e-04  0.000000   \n",
       "17   8.352500e-04  0.000028  3.685000e-02  0.001597  3.034200e-04  0.000012   \n",
       "18   4.249400e-04  0.000016  1.920200e-02  0.001126  1.543600e-04  0.000008   \n",
       "19   2.088900e-04  0.000000  8.980400e-03  0.000000  7.590000e-05  0.000000   \n",
       "20   7.270000e-05  0.008507  2.831900e-03  0.373420  2.350000e-05  0.003091   \n",
       "21   1.187500e-02  0.006862  5.699400e-01  0.395870  4.313100e-03  0.002882   \n",
       "22   2.903500e-04  0.001009  1.158300e-02  0.047861  9.410000e-05  0.000366   \n",
       "23   7.925900e-04  0.000047  3.543600e-02  0.002126  2.879100e-04  0.000018   \n",
       "24   2.511600e-04  0.000052  1.034900e-02  0.002246  8.670000e-05  0.000019   \n",
       "25   6.100000e-05  0.000727  2.604200e-03  0.040561  2.220000e-05  0.000306   \n",
       "26   8.531700e-03  0.000988  4.306000e-01  0.077713  3.098300e-03  0.000504   \n",
       "27   9.967200e-03  0.000618  4.643200e-01  0.030451  3.620400e-03  0.000236   \n",
       "28   2.710500e-04  0.000000  1.044700e-02  0.000000  8.960000e-05  0.000000   \n",
       "29   5.420000e-05  0.000000  2.307700e-03  0.000000  1.970000e-05  0.000000   \n",
       "..            ...       ...           ...       ...           ...       ...   \n",
       "128  8.400000e-05  0.000204  2.951400e-03  0.008890  2.560000e-05  0.000074   \n",
       "129  3.471000e-03  0.000000  1.481500e-01  0.000000  1.174100e-03  0.000000   \n",
       "130  2.195200e-04  0.000000  9.413600e-03  0.000000  7.970000e-05  0.000000   \n",
       "131  9.061200e-03  0.000767  4.141800e-01  0.048749  3.228100e-03  0.000355   \n",
       "132  9.733500e-03  0.000000  4.302900e-01  0.000000  3.536000e-03  0.000000   \n",
       "133  4.166100e-03  0.001581  1.820900e-01  0.084363  1.448600e-03  0.000647   \n",
       "134  2.728100e-04  0.000000  1.048900e-02  0.000000  8.960000e-05  0.000000   \n",
       "135  2.457800e-03  0.002500  1.206300e-01  0.161090  8.926100e-04  0.001146   \n",
       "136  5.220000e-05  0.000612  2.245600e-03  0.036691  1.900000e-05  0.000272   \n",
       "137  6.670000e-17  0.000052  2.780000e-12  0.002246  1.090000e-14  0.000019   \n",
       "138  7.030000e-05  0.003652  2.792000e-03  0.168750  2.420000e-05  0.001341   \n",
       "139  1.132700e-03  0.001033  5.196800e-02  0.049448  4.114400e-04  0.000397   \n",
       "140  6.156500e-04  0.000000  2.400700e-02  0.000000  2.016700e-04  0.000000   \n",
       "141  4.648300e-04  0.000051  2.012100e-02  0.002219  1.688700e-04  0.000019   \n",
       "142  2.033900e-03  0.000000  7.297700e-02  0.000000  6.226100e-04  0.000000   \n",
       "143  1.210600e-04  0.000000  3.586600e-03  0.000000  3.320000e-05  0.000000   \n",
       "144  1.318700e-03  0.002990  5.945800e-02  0.164510  4.789900e-04  0.001218   \n",
       "145  3.546100e-03  0.007501  1.659400e-01  0.460380  1.288000e-03  0.003275   \n",
       "146  7.300000e-05  0.000000  2.709900e-03  0.000000  2.390000e-05  0.000000   \n",
       "147  1.318000e-02  0.000031  6.041100e-01  0.001668  4.702100e-03  0.000013   \n",
       "148  6.470000e-05  0.002394  2.664300e-03  0.106750  2.170000e-05  0.000870   \n",
       "149  1.292700e-03  0.000000  5.592000e-02  0.000000  4.696200e-04  0.000000   \n",
       "150  7.900700e-04  0.000000  3.502900e-02  0.000000  2.870100e-04  0.000000   \n",
       "151  6.747500e-03  0.000076  3.089700e-01  0.005073  2.451000e-03  0.000036   \n",
       "152  5.220000e-05  0.000000  2.245600e-03  0.000000  1.900000e-05  0.000000   \n",
       "153  1.241200e-03  0.000038  5.504300e-02  0.001886  4.508800e-04  0.000015   \n",
       "154  5.310000e-05  0.000000  2.259600e-03  0.000000  1.930000e-05  0.000000   \n",
       "155  2.406400e-04  0.000209  9.879200e-03  0.008983  8.120000e-05  0.000076   \n",
       "156  6.240000e-05  0.000087  2.691300e-03  0.005478  2.270000e-05  0.000040   \n",
       "157  5.070000e-05  0.000052  2.252400e-03  0.002246  1.760000e-05  0.000019   \n",
       "\n",
       "     Class  \n",
       "0        0  \n",
       "1        1  \n",
       "2        1  \n",
       "3        0  \n",
       "4        0  \n",
       "5        1  \n",
       "6        0  \n",
       "7        0  \n",
       "8        0  \n",
       "9        0  \n",
       "10       0  \n",
       "11       0  \n",
       "12       1  \n",
       "13       1  \n",
       "14       1  \n",
       "15       0  \n",
       "16       0  \n",
       "17       0  \n",
       "18       0  \n",
       "19       0  \n",
       "20       1  \n",
       "21       0  \n",
       "22       1  \n",
       "23       0  \n",
       "24       1  \n",
       "25       0  \n",
       "26       1  \n",
       "27       0  \n",
       "28       1  \n",
       "29       1  \n",
       "..     ...  \n",
       "128      1  \n",
       "129      0  \n",
       "130      0  \n",
       "131      1  \n",
       "132      0  \n",
       "133      0  \n",
       "134      1  \n",
       "135      1  \n",
       "136      0  \n",
       "137      1  \n",
       "138      0  \n",
       "139      0  \n",
       "140      0  \n",
       "141      1  \n",
       "142      1  \n",
       "143      1  \n",
       "144      0  \n",
       "145      0  \n",
       "146      1  \n",
       "147      0  \n",
       "148      0  \n",
       "149      0  \n",
       "150      0  \n",
       "151      0  \n",
       "152      1  \n",
       "153      0  \n",
       "154      0  \n",
       "155      0  \n",
       "156      0  \n",
       "157      1  \n",
       "\n",
       "[158 rows x 477 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "sc = StandardScaler()\n",
    "X2 = sc.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "#lda = LDA(n_components = 15)\n",
    "#A = lda.fit_transform(X2,Y)\n",
    "#A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "def forward_selection(data, target, significance_level=0.05):\n",
    "    initial_features = data.columns.tolist()\n",
    "    best_features = []\n",
    "    while (len(initial_features)>0):\n",
    "        remaining_features = list(set(initial_features)-set(best_features))\n",
    "        new_pval = pd.Series(index=remaining_features)\n",
    "        for new_column in remaining_features:\n",
    "            model = sm.OLS(target, sm.add_constant(data[best_features+[new_column]])).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        min_p_value = new_pval.min()\n",
    "        if(min_p_value<significance_level):\n",
    "            best_features.append(new_pval.idxmin())\n",
    "        else:\n",
    "            break\n",
    "    return best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m=forward_selection(X1,Y)\n",
    "\n",
    "#idx,_,_ = MRMR.mrmr(X2, Y, n_selected_features=10)\n",
    "#features = X2[:, idx[0:10]]\n",
    "#features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select=  X1[m]\n",
    "#select.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selectX = sc.fit_transform(select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score ,roc_curve,auc\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=10,shuffle=True,random_state=0)\n",
    "pred_test_full =0\n",
    "cv_score =[]\n",
    "pr_score =[]\n",
    "re_score= []\n",
    "au_score =[]\n",
    "i=1\n",
    "for train_index,test_index in kf.split(X2,Y):\n",
    "    #print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "    xtr,xvl = X2[train_index],X2[test_index]\n",
    "    ytr,yvl = Y[train_index],Y[test_index]\n",
    "    #idx,_,_ = MRMR.mrmr(xtr, ytr, n_selected_features=20)\n",
    "    #features =xtr[:, idx[0:5]]\n",
    "    #features1 = xvl[:, idx[0:5]]\n",
    "    lda = LDA(n_components = 15)\n",
    "    A1 = lda.fit_transform(xtr,ytr)\n",
    "    A2 = lda.fit_transform(xvl,yvl)\n",
    "    #model\n",
    "    lr = LogisticRegression(random_state=0,class_weight=\"balanced\")\n",
    "    lr.fit(A1,ytr)\n",
    "    score = metrics.accuracy_score(yvl,lr.predict(A2))\n",
    "    score1 = metrics.precision_score(yvl,lr.predict(A2))\n",
    "    score2 = metrics.recall_score(yvl,lr.predict(A2))\n",
    "    score3 = metrics.roc_auc_score(yvl,lr.predict(A2))\n",
    "    #print('Accuracy score:',score)\n",
    "    #print('Precision score:',score1)\n",
    "    #print('Recall score:',score2)\n",
    "    #print('AUC score:',score3)\n",
    "    \n",
    "    \n",
    "    cv_score.append(score) \n",
    "    pr_score.append(score1)\n",
    "    re_score.append(score2)\n",
    "    au_score.append(score3)\n",
    "    #pred_test = lr.predict_proba(xvl)[:,1]\n",
    "    #pred_test_full +=pred_test\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\nMean Accuracy', 0.9366666666666668)\n",
      "('\\nMean Precision', 0.8904761904761905)\n",
      "('\\nMean Recall', 0.95)\n",
      "('\\nMean Auc', 0.9399999999999998)\n"
     ]
    }
   ],
   "source": [
    "print('\\nMean Accuracy',np.mean(cv_score))\n",
    "print('\\nMean Precision',np.mean(pr_score))\n",
    "print('\\nMean Recall',np.mean(re_score))\n",
    "print('\\nMean Auc',np.mean(au_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "from scipy import interp\n",
    "\n",
    "kf = StratifiedKFold(n_splits=10,shuffle=True,random_state=0)\n",
    "pred_test_full =0\n",
    "cv_score =[]\n",
    "pr_score =[]\n",
    "re_score= []\n",
    "au_score =[]\n",
    "\n",
    "fig1 = plt.figure(figsize=[12,12])\n",
    "ax1 = fig1.add_subplot(111,aspect = 'equal')\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0,1,100)\n",
    "i=1\n",
    "for train_index,test_index in kf.split(X2,Y):\n",
    "    #print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "    xtr,xvl = X2[train_index],X2[test_index]\n",
    "    ytr,yvl = Y[train_index],Y[test_index]\n",
    "    #idx,_,_ = MRMR.mrmr(xtr, ytr, n_selected_features=20)\n",
    "    #features =xtr[:, idx[0:5]]\n",
    "    #features1 = xvl[:, idx[0:5]]\n",
    "    lda = LDA(n_components = 15)\n",
    "    A1 = lda.fit_transform(xtr,ytr)\n",
    "    A2 = lda.fit_transform(xvl,yvl)\n",
    "    lr = LogisticRegression(random_state=0)\n",
    "    #model\n",
    "    prediction = lr.fit(A1,ytr).predict(A2)\n",
    "    fpr, tpr, t = roc_curve(yvl, prediction[:])\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "    i= i+1\n",
    "\n",
    "\n",
    "plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, color='blue',\n",
    "         label=r'Mean ROC (AUC = %0.2f )' % (mean_auc),lw=2, alpha=1)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('TDA-LR-T1 MRI ROC plot')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "pred_test_full =0\n",
    "cv_score =[]\n",
    "pr_score =[]\n",
    "re_score= []\n",
    "au_score =[]\n",
    "\n",
    "i=1\n",
    "for train_index,test_index in kf.split(X2,Y):\n",
    "    #print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "    xtr,xvl = X2[train_index],X2[test_index]\n",
    "    ytr,yvl = Y[train_index],Y[test_index]\n",
    "    lda = LDA(n_components = 15)\n",
    "    A1 = lda.fit_transform(xtr,ytr)\n",
    "    A2 = lda.fit_transform(xvl,yvl)\n",
    "    \n",
    "    #model\n",
    "    sv = svm.SVC(kernel='linear', C=1.5,class_weight=\"balanced\")\n",
    "    sv.fit(A1,ytr)\n",
    "    score = metrics.accuracy_score(yvl,sv.predict(A2))\n",
    "    score1 = metrics.precision_score(yvl,sv.predict(A2))\n",
    "    score2 = metrics.recall_score(yvl,sv.predict(A2))\n",
    "    score3 = metrics.roc_auc_score(yvl,sv.predict(A2))\n",
    "    #print('Accuracy score:',score)\n",
    "    #print('Precision score:',score1)\n",
    "    #print('Recall score:',score2)\n",
    "    #print('AUC score:',score3)\n",
    "    cv_score.append(score) \n",
    "    pr_score.append(score1)\n",
    "    re_score.append(score2)\n",
    "    au_score.append(score3)\n",
    "    #pred_test = lr.predict_proba(xvl)[:,1]\n",
    "    #pred_test_full +=pred_test\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\nMean Accuracy', 0.8744852941176472)\n",
      "('\\nMean Precision', 0.8990476190476191)\n",
      "('\\nMean Recall', 0.7733333333333333)\n",
      "('\\nMean Auc', 0.8521212121212122)\n"
     ]
    }
   ],
   "source": [
    "print('\\nMean Accuracy',np.mean(cv_score))\n",
    "print('\\nMean Precision',np.mean(pr_score))\n",
    "print('\\nMean Recall',np.mean(re_score))\n",
    "print('\\nMean Auc',np.mean(au_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "pred_test_full =0\n",
    "cv_score =[]\n",
    "pr_score =[]\n",
    "re_score= []\n",
    "au_score =[]\n",
    "\n",
    "i=1\n",
    "for train_index,test_index in kf.split(X2,Y):\n",
    "    #print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "    xtr,xvl = X2[train_index],X2[test_index]\n",
    "    ytr,yvl = Y[train_index],Y[test_index]\n",
    "    lda = LDA(n_components = 15)\n",
    "    A1 = lda.fit_transform(xtr,ytr)\n",
    "    A2 = lda.fit_transform(xvl,yvl)\n",
    "    \n",
    "    #model\n",
    "    rf = RandomForestClassifier(n_estimators=100,max_depth=3,random_state=0,class_weight=\"balanced\")\n",
    "    rf.fit(A1,ytr)\n",
    "    score = metrics.accuracy_score(yvl,rf.predict(A2))\n",
    "    score1 = metrics.precision_score(yvl,rf.predict(A2))\n",
    "    score2 = metrics.recall_score(yvl,rf.predict(A2))\n",
    "    score3 = metrics.roc_auc_score(yvl,rf.predict(A2))\n",
    "    #print('Accuracy score:',score)\n",
    "    #print('Precision score:',score1)\n",
    "    #print('Recall score:',score2)\n",
    "    #print('AUC score:',score3)\n",
    "    cv_score.append(score) \n",
    "    pr_score.append(score1)\n",
    "    re_score.append(score2)\n",
    "    au_score.append(score3)\n",
    "    #pred_test = lr.predict_proba(xvl)[:,1]\n",
    "    #pred_test_full +=pred_test\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\nMean Accuracy', 0.824436274509804)\n",
      "('\\nMean Precision', 0.8433333333333334)\n",
      "('\\nMean Recall', 0.6366666666666666)\n",
      "('\\nMean Auc', 0.7837878787878788)\n"
     ]
    }
   ],
   "source": [
    "print('\\nMean Accuracy',np.mean(cv_score))\n",
    "print('\\nMean Precision',np.mean(pr_score))\n",
    "print('\\nMean Recall',np.mean(re_score))\n",
    "print('\\nMean Auc',np.mean(au_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "pred_test_full =0\n",
    "cv_score =[]\n",
    "pr_score =[]\n",
    "re_score= []\n",
    "au_score =[]\n",
    "\n",
    "i=1\n",
    "for train_index,test_index in kf.split(X2,Y):\n",
    "    #print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "    xtr,xvl = X2[train_index],X2[test_index]\n",
    "    ytr,yvl = Y[train_index],Y[test_index]\n",
    "    lda = LDA(n_components = 15)\n",
    "    A1 = lda.fit_transform(xtr,ytr)\n",
    "    A2 = lda.fit_transform(xvl,yvl)\n",
    "    \n",
    "    #model\n",
    "    xg = xgb.XGBClassifier(colsample_bytree=1, objective ='binary:logistic',\n",
    "                    eval_metric= 'auc', min_child_weight=1, class_weight=\"balanced\",            \n",
    "                learning_rate=0.07,\n",
    "                 max_depth=3,\n",
    "                 #min_child_weight=1.5,\n",
    "                 n_estimators=1000,                                                                    \n",
    "                 #reg_alpha=0.45,\n",
    "                reg_lambda=0.1,\n",
    "                 subsample=0.8,scale_pos_weight=2,\n",
    "                 seed=42)\n",
    "    xg.fit(A1,ytr)\n",
    "    score = metrics.accuracy_score(yvl,xg.predict(A2))\n",
    "    score1 = metrics.precision_score(yvl,xg.predict(A2))\n",
    "    score2 = metrics.recall_score(yvl,xg.predict(A2))\n",
    "    score3 = metrics.roc_auc_score(yvl,xg.predict(A2))\n",
    "    #print('Accuracy score:',score)\n",
    "    #print('Precision score:',score1)\n",
    "    #print('Recall score:',score2)\n",
    "    #print('AUC score:',score3)\n",
    "    cv_score.append(score) \n",
    "    pr_score.append(score1)\n",
    "    re_score.append(score2)\n",
    "    au_score.append(score3)\n",
    "    #pred_test = lr.predict_proba(xvl)[:,1]\n",
    "    #pred_test_full +=pred_test\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\nMean Accuracy', 0.8223529411764705)\n",
      "('\\nMean Precision', 0.8058333333333334)\n",
      "('\\nMean Recall', 0.7966666666666666)\n",
      "('\\nMean Auc', 0.8187878787878787)\n"
     ]
    }
   ],
   "source": [
    "print('\\nMean Accuracy',np.mean(cv_score))\n",
    "print('\\nMean Precision',np.mean(pr_score))\n",
    "print('\\nMean Recall',np.mean(re_score))\n",
    "print('\\nMean Auc',np.mean(au_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "pred_test_full =0\n",
    "cv_score =[]\n",
    "pr_score =[]\n",
    "re_score= []\n",
    "au_score =[]\n",
    "\n",
    "i=1\n",
    "for train_index,test_index in kf.split(X2,Y):\n",
    "    #print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "    xtr,xvl = X2[train_index],X2[test_index]\n",
    "    ytr,yvl = Y[train_index],Y[test_index]\n",
    "    lda = LDA(n_components = 15)\n",
    "    A1 = lda.fit_transform(xtr,ytr)\n",
    "    A2 = lda.fit_transform(xvl,yvl)\n",
    "    \n",
    "    #model\n",
    "    dt = tree.DecisionTreeClassifier(class_weight=\"balanced\")\n",
    "    dt.fit(A1,ytr)\n",
    "    score = metrics.accuracy_score(yvl,dt.predict(A2))\n",
    "    score1 = metrics.precision_score(yvl,dt.predict(A2))\n",
    "    score2 = metrics.recall_score(yvl,dt.predict(A2))\n",
    "    score3 = metrics.roc_auc_score(yvl,dt.predict(A2))\n",
    "    #print('Accuracy score:',score)\n",
    "    #print('Precision score:',score1)\n",
    "    #print('Recall score:',score2)\n",
    "    #print('AUC score:',score3)\n",
    "    cv_score.append(score) \n",
    "    pr_score.append(score1)\n",
    "    re_score.append(score2)\n",
    "    au_score.append(score3)\n",
    "    #pred_test = lr.predict_proba(xvl)[:,1]\n",
    "    #pred_test_full +=pred_test\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\nMean Accuracy', 0.824436274509804)\n",
      "('\\nMean Precision', 0.8433333333333334)\n",
      "('\\nMean Recall', 0.6366666666666666)\n",
      "('\\nMean Auc', 0.7837878787878788)\n"
     ]
    }
   ],
   "source": [
    "print('\\nMean Accuracy',np.mean(cv_score))\n",
    "print('\\nMean Precision',np.mean(pr_score))\n",
    "print('\\nMean Recall',np.mean(re_score))\n",
    "print('\\nMean Auc',np.mean(au_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158, 476)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X11.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X21 = sc.fit_transform(X11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx,_,_ = MRMR.mrmr(X21,Y, n_selected_features=20)\n",
    "#featuresX =xtr[:, idx[0:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=5,shuffle=True,random_state=0)\n",
    "pred_test_full =0\n",
    "cv_score =[]\n",
    "pr_score =[]\n",
    "re_score= []\n",
    "au_score =[]\n",
    "i=1\n",
    "for train_index,test_index in kf.split(X21,Y):\n",
    "    #print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "    xtr,xvl = X21[train_index],X21[test_index]\n",
    "    ytr,yvl = Y[train_index],Y[test_index]\n",
    "    idx,_,_ = MRMR.mrmr(xtr, ytr, n_selected_features=20)\n",
    "    features =xtr[:, idx[0:5]]\n",
    "    features1 = xvl[:, idx[0:5]]\n",
    "    #lda = LDA(n_components = 15)\n",
    "    #A1 = lda.fit_transform(xtr,ytr)\n",
    "    #A2 = lda.fit_transform(xvl,yvl)\n",
    "    #model\n",
    "    lr = LogisticRegression(random_state=0)\n",
    "    lr.fit(features,ytr)\n",
    "    score = metrics.accuracy_score(yvl,lr.predict(features1))\n",
    "    score1 = metrics.precision_score(yvl,lr.predict(features1))\n",
    "    score2 = metrics.recall_score(yvl,lr.predict(features1))\n",
    "    score3 = metrics.roc_auc_score(yvl,lr.predict(features1))\n",
    "    #print('Accuracy score:',score)\n",
    "    #print('Precision score:',score1)\n",
    "    #print('Recall score:',score2)\n",
    "    #print('AUC score:',score3)\n",
    "    \n",
    "    \n",
    "    cv_score.append(score) \n",
    "    pr_score.append(score1)\n",
    "    re_score.append(score2)\n",
    "    au_score.append(score3)\n",
    "    #pred_test = lr.predict_proba(xvl)[:,1]\n",
    "    #pred_test_full +=pred_test\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\nMean Accuracy', 0.6208088954056695)\n",
      "('\\nMean Precision', 0.05)\n",
      "('\\nMean Recall', 0.016666666666666666)\n",
      "('\\nMean Auc', 0.4885714285714286)\n"
     ]
    }
   ],
   "source": [
    "print('\\nMean Accuracy',np.mean(cv_score))\n",
    "print('\\nMean Precision',np.mean(pr_score))\n",
    "print('\\nMean Recall',np.mean(re_score))\n",
    "print('\\nMean Auc',np.mean(au_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "pred_test_full =0\n",
    "cv_score =[]\n",
    "pr_score =[]\n",
    "re_score= []\n",
    "au_score =[]\n",
    "\n",
    "i=1\n",
    "for train_index,test_index in kf.split(X21,Y):\n",
    "    #print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "    xtr,xvl = X21[train_index],X21[test_index]\n",
    "    ytr,yvl = Y[train_index],Y[test_index]\n",
    "    idx,_,_ = MRMR.mrmr(xtr, ytr, n_selected_features=20)\n",
    "    features =xtr[:, idx[0:5]]\n",
    "    features1 = xvl[:, idx[0:5]]\n",
    "    \n",
    "    #model\n",
    "    sv = svm.SVC(kernel='linear', C=1.5)\n",
    "    sv.fit(features,ytr)\n",
    "    score = metrics.accuracy_score(yvl,sv.predict(features1))\n",
    "    score1 = metrics.precision_score(yvl,sv.predict(features1))\n",
    "    score2 = metrics.recall_score(yvl,sv.predict(features1))\n",
    "    score3 = metrics.roc_auc_score(yvl,sv.predict(features1))\n",
    "    #print('Accuracy score:',score)\n",
    "    #print('Precision score:',score1)\n",
    "    #print('Recall score:',score2)\n",
    "    #print('AUC score:',score3)\n",
    "    cv_score.append(score) \n",
    "    pr_score.append(score1)\n",
    "    re_score.append(score2)\n",
    "    au_score.append(score3)\n",
    "    #pred_test = lr.predict_proba(xvl)[:,1]\n",
    "    #pred_test_full +=pred_test\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\nMean Accuracy', 0.6393695014662756)\n",
      "('\\nMean Precision', 0.0)\n",
      "('\\nMean Recall', 0.0)\n",
      "('\\nMean Auc', 0.5)\n"
     ]
    }
   ],
   "source": [
    "print('\\nMean Accuracy',np.mean(cv_score))\n",
    "print('\\nMean Precision',np.mean(pr_score))\n",
    "print('\\nMean Recall',np.mean(re_score))\n",
    "print('\\nMean Auc',np.mean(au_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "pred_test_full =0\n",
    "cv_score =[]\n",
    "pr_score =[]\n",
    "re_score= []\n",
    "au_score =[]\n",
    "\n",
    "i=1\n",
    "for train_index,test_index in kf.split(X21,Y):\n",
    "    #print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "    xtr,xvl = X21[train_index],X21[test_index]\n",
    "    ytr,yvl = Y[train_index],Y[test_index]\n",
    "    idx,_,_ = MRMR.mrmr(xtr, ytr, n_selected_features=20)\n",
    "    features =xtr[:, idx[0:5]]\n",
    "    features1 = xvl[:, idx[0:5]]\n",
    "    #model\n",
    "    rf = RandomForestClassifier(n_estimators=100,max_depth=3,random_state=42)\n",
    "    rf.fit(features,ytr)\n",
    "    score = metrics.accuracy_score(yvl,rf.predict(features1))\n",
    "    score1 = metrics.precision_score(yvl,rf.predict(features1))\n",
    "    score2 = metrics.recall_score(yvl,rf.predict(features1))\n",
    "    score3 = metrics.roc_auc_score(yvl,rf.predict(features1))\n",
    "    #print('Accuracy score:',score)\n",
    "    #print('Precision score:',score1)\n",
    "    #print('Recall score:',score2)\n",
    "    #print('AUC score:',score3)\n",
    "    cv_score.append(score) \n",
    "    pr_score.append(score1)\n",
    "    re_score.append(score2)\n",
    "    au_score.append(score3)\n",
    "    #pred_test = lr.predict_proba(xvl)[:,1]\n",
    "    #pred_test_full +=pred_test\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\nMean Accuracy', 0.6143450635386118)\n",
      "('\\nMean Precision', 0.2523809523809524)\n",
      "('\\nMean Recall', 0.12272727272727273)\n",
      "('\\nMean Auc', 0.5068398268398269)\n"
     ]
    }
   ],
   "source": [
    "print('\\nMean Accuracy',np.mean(cv_score))\n",
    "print('\\nMean Precision',np.mean(pr_score))\n",
    "print('\\nMean Recall',np.mean(re_score))\n",
    "print('\\nMean Auc',np.mean(au_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_full =0\n",
    "cv_score =[]\n",
    "pr_score =[]\n",
    "re_score= []\n",
    "au_score =[]\n",
    "\n",
    "i=1\n",
    "for train_index,test_index in kf.split(X21,Y):\n",
    "    #print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "    xtr,xvl = X21[train_index],X21[test_index]\n",
    "    ytr,yvl = Y[train_index],Y[test_index]\n",
    "    idx,_,_ = MRMR.mrmr(xtr, ytr, n_selected_features=20)\n",
    "    features =xtr[:, idx[0:5]]\n",
    "    features1 = xvl[:, idx[0:5]]\n",
    "    \n",
    "    #model\n",
    "    dt = tree.DecisionTreeClassifier()\n",
    "    dt.fit(features,ytr)\n",
    "    score = metrics.accuracy_score(yvl,dt.predict(features1))\n",
    "    score1 = metrics.precision_score(yvl,dt.predict(features1))\n",
    "    score2 = metrics.recall_score(yvl,dt.predict(features1))\n",
    "    score3 = metrics.roc_auc_score(yvl,dt.predict(features1))\n",
    "    #print('Accuracy score:',score)\n",
    "    #print('Precision score:',score1)\n",
    "    #print('Recall score:',score2)\n",
    "    #print('AUC score:',score3)\n",
    "    cv_score.append(score) \n",
    "    pr_score.append(score1)\n",
    "    re_score.append(score2)\n",
    "    au_score.append(score3)\n",
    "    #pred_test = lr.predict_proba(xvl)[:,1]\n",
    "    #pred_test_full +=pred_test\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\nMean Accuracy', 0.607893450635386)\n",
      "('\\nMean Precision', 0.4866666666666667)\n",
      "('\\nMean Recall', 0.35151515151515156)\n",
      "('\\nMean Auc', 0.5521861471861472)\n"
     ]
    }
   ],
   "source": [
    "print('\\nMean Accuracy',np.mean(cv_score))\n",
    "print('\\nMean Precision',np.mean(pr_score))\n",
    "print('\\nMean Recall',np.mean(re_score))\n",
    "print('\\nMean Auc',np.mean(au_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "pred_test_full =0\n",
    "cv_score =[]\n",
    "pr_score =[]\n",
    "re_score= []\n",
    "au_score =[]\n",
    "\n",
    "i=1\n",
    "for train_index,test_index in kf.split(X21,Y):\n",
    "    #print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "    xtr,xvl = X21[train_index],X21[test_index]\n",
    "    ytr,yvl = Y[train_index],Y[test_index]\n",
    "    idx,_,_ = MRMR.mrmr(xtr, ytr, n_selected_features=20)\n",
    "    features =xtr[:, idx[0:20]]\n",
    "    features1 = xvl[:, idx[0:20]]\n",
    "    \n",
    "    #model\n",
    "    xg = xgb.XGBClassifier(colsample_bytree=0.9, objective ='binary:logistic',\n",
    "                    eval_metric= 'error', min_child_weight=1, class_weight=\"balanced\" ,            \n",
    "                learning_rate=0.06,\n",
    "                 max_depth=7,\n",
    "                 #min_child_weight=1.5,\n",
    "                 n_estimators=1000,                                                                    \n",
    "                 #reg_alpha=0.45,\n",
    "                reg_lambda=0.1,\n",
    "                 subsample=0.7,scale_pos_weight=2,\n",
    "                 seed=42)\n",
    "    xg.fit(features,ytr)\n",
    "    score = metrics.accuracy_score(yvl,xg.predict(features1))\n",
    "    score1 = metrics.precision_score(yvl,xg.predict(features1))\n",
    "    score2 = metrics.recall_score(yvl,xg.predict(features1))\n",
    "    score3 = metrics.roc_auc_score(yvl,xg.predict(features1))\n",
    "    #print('Accuracy score:',score)\n",
    "    #print('Precision score:',score1)\n",
    "    #print('Recall score:',score2)\n",
    "    #print('AUC score:',score3)\n",
    "    cv_score.append(score) \n",
    "    pr_score.append(score1)\n",
    "    re_score.append(score2)\n",
    "    au_score.append(score3)\n",
    "    #pred_test = lr.predict_proba(xvl)[:,1]\n",
    "    #pred_test_full +=pred_test\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\nMean Accuracy', 0.5563049853372434)\n",
      "('\\nMean Precision', 0.386984126984127)\n",
      "('\\nMean Recall', 0.35)\n",
      "('\\nMean Auc', 0.510952380952381)\n"
     ]
    }
   ],
   "source": [
    "print('\\nMean Accuracy',np.mean(cv_score))\n",
    "print('\\nMean Precision',np.mean(pr_score))\n",
    "print('\\nMean Recall',np.mean(re_score))\n",
    "print('\\nMean Auc',np.mean(au_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_elimination(data, target,significance_level = 0.05):\n",
    "    features = data.columns.tolist()\n",
    "    while(len(features)>0):\n",
    "        features_with_constant = sm.add_constant(data[features])\n",
    "        p_values = sm.OLS(target, features_with_constant).fit().pvalues[1:]\n",
    "        max_p_value = p_values.max()\n",
    "        if(max_p_value >= significance_level):\n",
    "            excluded_feature = p_values.idxmax()\n",
    "            features.remove(excluded_feature)\n",
    "        else:\n",
    "            break \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m1=backward_elimination(X1,Y)\n",
    "#sel1=  X1[m1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stepwise_selection(data, target,SL_in=0.05,SL_out = 0.05):\n",
    "    initial_features = data.columns.tolist()\n",
    "    best_features = []\n",
    "    while (len(initial_features)>0):\n",
    "        remaining_features = list(set(initial_features)-set(best_features))\n",
    "        new_pval = pd.Series(index=remaining_features)\n",
    "        for new_column in remaining_features:\n",
    "            model = sm.OLS(target, sm.add_constant(data[best_features+[new_column]])).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        min_p_value = new_pval.min()\n",
    "        if(min_p_value<SL_in):\n",
    "            best_features.append(new_pval.idxmin())\n",
    "            while(len(best_features)>0):\n",
    "                best_features_with_constant = sm.add_constant(data[best_features])\n",
    "                p_values = sm.OLS(target, best_features_with_constant).fit().pvalues[1:]\n",
    "                max_p_value = p_values.max()\n",
    "                if(max_p_value >= SL_out):\n",
    "                    excluded_feature = p_values.idxmax()\n",
    "                    best_features.remove(excluded_feature)\n",
    "                else:\n",
    "                    break \n",
    "        else:\n",
    "            break\n",
    "    return best_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#m2=stepwise_selection(X1,Y)\n",
    "#sel2=  X1[m2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_selection(data, target, significance_level=0.05):\n",
    "    initial_features = data.columns.tolist()\n",
    "    best_features = []\n",
    "    while (len(initial_features)>0):\n",
    "        remaining_features = list(set(initial_features)-set(best_features))\n",
    "        new_pval = pd.Series(index=remaining_features)\n",
    "        for new_column in remaining_features:\n",
    "            model = sm.OLS(target, sm.add_constant(data[best_features+[new_column]])).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        min_p_value = new_pval.min()\n",
    "        if(min_p_value<significance_level):\n",
    "            best_features.append(new_pval.idxmin())\n",
    "        else:\n",
    "            break\n",
    "    return best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/statsmodels/base/model.py:1100: RuntimeWarning: invalid value encountered in divide\n",
      "  return self.params / self.bse\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/scipy/stats/_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/scipy/stats/_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/scipy/stats/_distn_infrastructure.py:1831: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n"
     ]
    }
   ],
   "source": [
    "m3=backward_elimination(X1,Y)\n",
    "sel3=  X1[m3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sel1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sel2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158, 0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 feature(s) (shape=(158, 0)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-8b4055b4e50b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msel_tran\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msel3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#selectA = lda.fit_transform(sel3,Y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/preprocessing/data.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    615\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/preprocessing/data.pyc\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    639\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[1;32m    640\u001b[0m                         \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m                         force_all_finite='allow-nan')\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rahulpaul/Library/Python/2.7/lib/python/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    583\u001b[0m                              \u001b[0;34m\" a minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m                              % (n_features, shape_repr, ensure_min_features,\n\u001b[0;32m--> 585\u001b[0;31m                                 context))\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwarn_on_dtype\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdtype_orig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdtype_orig\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 feature(s) (shape=(158, 0)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "sel_tran = sc.fit_transform(sel3)\n",
    "#selectA = lda.fit_transform(sel3,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=5,shuffle=True,random_state=0)\n",
    "pred_test_full =0\n",
    "cv_score =[]\n",
    "pr_score =[]\n",
    "re_score= []\n",
    "au_score =[]\n",
    "i=1\n",
    "for train_index,test_index in kf.split(sel_tran,Y):\n",
    "    #print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "    xtr,xvl = sel_tran[train_index],sel_tran[test_index]\n",
    "    ytr,yvl = Y[train_index],Y[test_index]\n",
    "    #idx,_,_ = MRMR.mrmr(xtr, ytr, n_selected_features=20)\n",
    "    #features =xtr[:, idx[0:5]]\n",
    "    #features1 = xvl[:, idx[0:5]]\n",
    "    #lda = LDA(n_components = 15)\n",
    "    #A1 = lda.fit_transform(xtr,ytr)\n",
    "    #A2 = lda.fit_transform(xvl,yvl)\n",
    "    #model\n",
    "    lr = LogisticRegression(random_state=0)\n",
    "    lr.fit(xtr,ytr)\n",
    "    score = metrics.accuracy_score(yvl,lr.predict(xvl))\n",
    "    score1 = metrics.precision_score(yvl,lr.predict(xvl))\n",
    "    score2 = metrics.recall_score(yvl,lr.predict(xvl))\n",
    "    score3 = metrics.roc_auc_score(yvl,lr.predict(xvl))\n",
    "    #print('Accuracy score:',score)\n",
    "    #print('Precision score:',score1)\n",
    "    #print('Recall score:',score2)\n",
    "    #print('AUC score:',score3)\n",
    "    \n",
    "    \n",
    "    cv_score.append(score) \n",
    "    pr_score.append(score1)\n",
    "    re_score.append(score2)\n",
    "    au_score.append(score3)\n",
    "    #pred_test = lr.predict_proba(xvl)[:,1]\n",
    "    #pred_test_full +=pred_test\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nMean Accuracy',np.mean(cv_score))\n",
    "print('\\nMean Precision',np.mean(pr_score))\n",
    "print('\\nMean Recall',np.mean(re_score))\n",
    "print('\\nMean Auc',np.mean(au_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_full =0\n",
    "cv_score =[]\n",
    "pr_score =[]\n",
    "re_score= []\n",
    "au_score =[]\n",
    "\n",
    "i=1\n",
    "for train_index,test_index in kf.split(sel_tran,Y):\n",
    "    #print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "    xtr,xvl = sel_tran[train_index],sel_tran[test_index]\n",
    "    ytr,yvl = Y[train_index],Y[test_index]\n",
    "    lda = LDA(n_components = 15)\n",
    "    A1 = lda.fit_transform(xtr,ytr)\n",
    "    A2 = lda.fit_transform(xvl,yvl)\n",
    "    \n",
    "    #model\n",
    "    sv = svm.SVC(kernel='linear', C=1.5)\n",
    "    sv.fit(xtr,ytr)\n",
    "    score = metrics.accuracy_score(yvl,sv.predict(xvl))\n",
    "    score1 = metrics.precision_score(yvl,sv.predict(xvl))\n",
    "    score2 = metrics.recall_score(yvl,sv.predict(xvl))\n",
    "    score3 = metrics.roc_auc_score(yvl,sv.predict(xvl))\n",
    "    #print('Accuracy score:',score)\n",
    "    #print('Precision score:',score1)\n",
    "    #print('Recall score:',score2)\n",
    "    #print('AUC score:',score3)\n",
    "    cv_score.append(score) \n",
    "    pr_score.append(score1)\n",
    "    re_score.append(score2)\n",
    "    au_score.append(score3)\n",
    "    #pred_test = lr.predict_proba(xvl)[:,1]\n",
    "    #pred_test_full +=pred_test\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nMean Accuracy',np.mean(cv_score))\n",
    "print('\\nMean Precision',np.mean(pr_score))\n",
    "print('\\nMean Recall',np.mean(re_score))\n",
    "print('\\nMean Auc',np.mean(au_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_full =0\n",
    "cv_score =[]\n",
    "pr_score =[]\n",
    "re_score= []\n",
    "au_score =[]\n",
    "\n",
    "i=1\n",
    "for train_index,test_index in kf.split(sel_tran,Y):\n",
    "    #print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "    xtr,xvl = sel_tran[train_index],sel_tran[test_index]\n",
    "    ytr,yvl = Y[train_index],Y[test_index]\n",
    "    lda = LDA(n_components = 15)\n",
    "    A1 = lda.fit_transform(xtr,ytr)\n",
    "    A2 = lda.fit_transform(xvl,yvl)\n",
    "    \n",
    "    #model\n",
    "    rf = RandomForestClassifier(n_estimators=250,max_depth=2,random_state=0)\n",
    "    rf.fit(xtr,ytr)\n",
    "    score = metrics.accuracy_score(yvl,rf.predict(xvl))\n",
    "    score1 = metrics.precision_score(yvl,rf.predict(xvl))\n",
    "    score2 = metrics.recall_score(yvl,rf.predict(xvl))\n",
    "    score3 = metrics.roc_auc_score(yvl,rf.predict(xvl))\n",
    "    #print('Accuracy score:',score)\n",
    "    #print('Precision score:',score1)\n",
    "    #print('Recall score:',score2)\n",
    "    #print('AUC score:',score3)\n",
    "    cv_score.append(score) \n",
    "    pr_score.append(score1)\n",
    "    re_score.append(score2)\n",
    "    au_score.append(score3)\n",
    "    #pred_test = lr.predict_proba(xvl)[:,1]\n",
    "    #pred_test_full +=pred_test\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nMean Accuracy',np.mean(cv_score))\n",
    "print('\\nMean Precision',np.mean(pr_score))\n",
    "print('\\nMean Recall',np.mean(re_score))\n",
    "print('\\nMean Auc',np.mean(au_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_full =0\n",
    "cv_score =[]\n",
    "pr_score =[]\n",
    "re_score= []\n",
    "au_score =[]\n",
    "\n",
    "i=1\n",
    "for train_index,test_index in kf.split(sel_tran,Y):\n",
    "    #print('{} of KFold {}'.format(i,kf.n_splits))\n",
    "    xtr,xvl = sel_tran[train_index],sel_tran[test_index]\n",
    "    ytr,yvl = Y[train_index],Y[test_index]\n",
    "    lda = LDA(n_components = 15)\n",
    "    A1 = lda.fit_transform(xtr,ytr)\n",
    "    A2 = lda.fit_transform(xvl,yvl)\n",
    "    \n",
    "    #model\n",
    "    dt = tree.DecisionTreeClassifier()\n",
    "    dt.fit(xtr,ytr)\n",
    "    score = metrics.accuracy_score(yvl,dt.predict(xvl))\n",
    "    score1 = metrics.precision_score(yvl,dt.predict(xvl))\n",
    "    score2 = metrics.recall_score(yvl,dt.predict(xvl))\n",
    "    score3 = metrics.roc_auc_score(yvl,dt.predict(xvl))\n",
    "    #print('Accuracy score:',score)\n",
    "    #print('Precision score:',score1)\n",
    "    #print('Recall score:',score2)\n",
    "    #print('AUC score:',score3)\n",
    "    cv_score.append(score) \n",
    "    pr_score.append(score1)\n",
    "    re_score.append(score2)\n",
    "    au_score.append(score3)\n",
    "    #pred_test = lr.predict_proba(xvl)[:,1]\n",
    "    #pred_test_full +=pred_test\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nMean Accuracy',np.mean(cv_score))\n",
    "print('\\nMean Precision',np.mean(pr_score))\n",
    "print('\\nMean Recall',np.mean(re_score))\n",
    "print('\\nMean Auc',np.mean(au_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
